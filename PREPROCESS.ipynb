{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79e09aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN         deed reason # earthquak may allah forgiv u   \n",
      "1   4     NaN      NaN             forest fire near la rong sask . canada   \n",
      "2   5     NaN      NaN  resid ask shelter place ' notifi offic . evacu...   \n",
      "3   6     NaN      NaN  13,000 peopl receiv # wildfir evacu order cali...   \n",
      "4   7     NaN      NaN  got sent photo rubi # alaska smoke # wildfir p...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "\n",
      "Test Dataset:\n",
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "#preprocess step\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\test.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# FUNCTION FOR TEXT PREPROCESSING\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "        \n",
    "       \n",
    "        # JOIN THE TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# TEXT COLUMN TO PREPROCESS \n",
    "text_columns = [\n",
    "    \"id\",\n",
    "    \"keyword\",\n",
    "    \"location\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "# TRAIN DATASET PREPROCESSING\n",
    "for column in text_columns:\n",
    "    train_df[column] = train_df[column].apply(preprocess_text)\n",
    "\n",
    "# TEST DATASET PREPROCESSING\n",
    "for column in text_columns[:-1]:\n",
    "    test_df[column] = test_df[column].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(\"Train Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123d8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dade788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform  text vectorization....TF-IDF\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# FUNCTION FOR TEXT PREPROCESSING\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        \n",
    "       \n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        \n",
    "      \n",
    "        text = text.lower()\n",
    "        \n",
    "       \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "       \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "       \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "        \n",
    "       \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "        \n",
    "        # JOIN THE TOKENS BACK TO ORGINAL TEXT\n",
    "        preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return str(text)\n",
    "\n",
    "# TEXT COLUMN TO PREPROCESS\n",
    "text_columns = [\n",
    "    \"id\",\n",
    "    \"keyword\",\n",
    "    \"location\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "# TRAIN DATASET PREPROCESSING\n",
    "for column in text_columns:\n",
    "    train_df[column] = train_df[column].apply(preprocess_text)\n",
    "\n",
    "#TEST DATASET PREPROCESSING\n",
    "for column in text_columns[:-1]:\n",
    "    test_df[column] = test_df[column].apply(preprocess_text)\n",
    "\n",
    "# COMBINE PREPROCESSED TEXT COLUMN INTO A SINGLE COLUMN\n",
    "train_df['preprocessed_text'] = train_df[text_columns[:-1]].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_df['preprocessed_text'] = test_df[text_columns[:-1]].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# TEXT VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectorized_text = vectorizer.fit_transform(train_df['preprocessed_text'])\n",
    "test_vectorized_text = vectorizer.transform(test_df['preprocessed_text'])\n",
    "\n",
    "# GET FEATURE NAMES\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# CONVERT THE VECTORIZED TEXT TO DATAFRAME\n",
    "train_vectorized_df = pd.DataFrame(train_vectorized_text.toarray(), columns=feature_names)\n",
    "test_vectorized_df = pd.DataFrame(test_vectorized_text.toarray(), columns=feature_names)\n",
    "\n",
    "# DROP NON NUMERIC COLUMN  FROM train_df\n",
    "train_df = train_df.select_dtypes(include='number')\n",
    "\n",
    "\n",
    "# CONCATENATE THE VECTORIZED DATAFRAME  WITH THE ORGINAL DATAFRAME\n",
    "train_df = pd.concat([train_df, train_vectorized_df], axis=1)\n",
    "test_df = pd.concat([test_df, test_vectorized_df], axis=1)\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
