{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1ea238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7905449770190414\n"
     ]
    }
   ],
   "source": [
    "# PERFROM LOGISTICS REGRESSION TO PREDICT WHICH TWEETS ARE ABOUT  THE REAL  DISASTERS AND WHICH ONES ARE NOT \n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\test.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# FUNCTION FOR TEXT PREPROCESSING\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        \n",
    "     \n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        \n",
    "       \n",
    "        text = text.lower()\n",
    "        \n",
    "       \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "       \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "       \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "        \n",
    "      \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "        \n",
    "       \n",
    "        # JOIN THE TOKEN BACK INTO TEXT\n",
    "        preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# TEXT COLUMN TO PREPROCESS \n",
    "text_columns = [\n",
    "    \"id\",\n",
    "    \"keyword\",\n",
    "    \"location\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "# TRAIN DATASET PREPROCESSING\n",
    "for column in text_columns:\n",
    "    train_df[column] = train_df[column].apply(preprocess_text)\n",
    "\n",
    "# TEST DATASET PREPROCESSING\n",
    "for column in text_columns[:-1]:\n",
    "    test_df[column] = test_df[column].apply(preprocess_text)\n",
    "\n",
    "# TRAINING DATA \n",
    "X = train_df['text']\n",
    "y = train_df['target']\n",
    "\n",
    "# SPLIT THE DATA INTO TRAINING AND VALIDATION SETS\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TEXT VECTORIZATIONS\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "\n",
    "# TRAIN MODEL\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# PREDICTION ON VALIDATION SET\n",
    "val_predictions = model.predict(X_val_vectorized)\n",
    "\n",
    "# CALCULATE ACCURACY ON VALIDATITY SETS\n",
    "accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# PREDICTION ON THE TEST SET\n",
    "test_predictions = model.predict(vectorizer.transform(test_df['text']))\n",
    "\n",
    "#MAKE SUBMISSION\n",
    "test_df['target'] = test_predictions\n",
    "\n",
    "\n",
    "test_df[['id', 'target']].to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d20714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c02923ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions:\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#perform naive bays model.....TO PREDICT WHICH TWEETS ARE ABOUT  THE REAL  DISASTERS AND WHICH ONES ARE NOT \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "train_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\train.csv\"\n",
    "test_path = r\"C:\\Users\\nh013\\Desktop\\NLP PROCESSING WITH DISESTER TWEETS\\test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "\n",
    "# FUNCTION FOR TEXT PREPROCESSING\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        \n",
    "       \n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "        \n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "       \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "       \n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "       \n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "        \n",
    "       \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
    "        \n",
    "        # JOIN THE TOKENS BACK TO ORGINAL TEXT\n",
    "        preprocessed_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return str(text)\n",
    "\n",
    "# TEXT COLUMN TO PREPROCESS\n",
    "text_columns = [\n",
    "    \"id\",\n",
    "    \"keyword\",\n",
    "    \"location\",\n",
    "    \"text\"\n",
    "]\n",
    "\n",
    "# TRAIN DATASET PREPROCESSING\n",
    "for column in text_columns:\n",
    "    train_df[column] = train_df[column].apply(preprocess_text)\n",
    "\n",
    "#TEST DATASET PREPROCESSING\n",
    "for column in text_columns[:-1]:\n",
    "    test_df[column] = test_df[column].apply(preprocess_text)\n",
    "\n",
    "# COMBINE PREPROCESSED TEXT COLUMN INTO A SINGLE COLUMN\n",
    "train_df['preprocessed_text'] = train_df[text_columns[:-1]].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_df['preprocessed_text'] = test_df[text_columns[:-1]].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# TEXT VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectorized_text = vectorizer.fit_transform(train_df['preprocessed_text'])\n",
    "test_vectorized_text = vectorizer.transform(test_df['preprocessed_text'])\n",
    "\n",
    "# GET FEATURE NAMES\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# CONVERT THE VECTORIZED TEXT TO DATAFRAME\n",
    "train_vectorized_df = pd.DataFrame(train_vectorized_text.toarray(), columns=feature_names)\n",
    "test_vectorized_df = pd.DataFrame(test_vectorized_text.toarray(), columns=feature_names)\n",
    "\n",
    "# DROP NON NUMERIC COLUMN  FROM train_df\n",
    "train_df = train_df.select_dtypes(include='number')\n",
    "\n",
    "\n",
    "# CONCATENATE THE VECTORIZED DATAFRAME  WITH THE ORGINAL DATAFRAME\n",
    "train_df = pd.concat([train_df, train_vectorized_df], axis=1)\n",
    "test_df = pd.concat([test_df, test_vectorized_df], axis=1)\n",
    "\n",
    "# LET'S SPLIT FEATURE AND TARGET \n",
    "X_train = train_df.drop([\"target\"], axis=1)\n",
    "y_train = train_df[\"target\"]\n",
    "\n",
    "# TRAIN MODEL\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# DROP NON NUMERIC COLUMN FROM test_df\n",
    "test_df = test_df.select_dtypes(include='number')\n",
    "\n",
    "# PREDICTION ON THE TEST SET\n",
    "test_predictions = naive_bayes.predict(test_df)\n",
    "\n",
    "# EVALUATE\n",
    "print(\"Test Predictions:\")\n",
    "print(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b3d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
